{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "91DCIRVJ_VQG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMOnlRkMBDXO",
        "outputId": "cdef941c-7b3c-4c54-a518-3c731cfdc619"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "yU6A2mivBICb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epORYb7cBLWd",
        "outputId": "a3e8717e-d04f-4355-815e-10f7ff918132"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qCE6PLWBNB7",
        "outputId": "df26b19e-876d-4db1-b6bb-e8e370147c93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqO5PelSBTCa",
        "outputId": "cc315de7-6047-4a04-9093-498eecd9e4da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "-YQAxkVcBWiv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jpA4xZ0c_VQJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gzip\n",
        "import random\n",
        "\n",
        "class Index:\n",
        "    def __init__(self) -> None:\n",
        "        self.index = defaultdict(dict)\n",
        "        self.embeddings = None\n",
        "        self.doc_lengths = {}\n",
        "        self.avgdl = 0\n",
        "        self.idf = {}\n",
        "        self.cf = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_text(text):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        ps = PorterStemmer()\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [ps.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "        return tokens\n",
        "\n",
        "    def load_file(self, file_name):\n",
        "        file_extension = file_name.split('.')[-1].lower()\n",
        "\n",
        "        if file_extension == 'csv':\n",
        "            self.docs = pd.read_csv(file_name)\n",
        "        elif file_extension == 'tsv':\n",
        "            self.docs = pd.read_csv(file_name, delimiter='\\t',header=None)\n",
        "            self.docs.columns = ['pid', 'passage']\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Supported formats: CSV (.csv) and TSV (.tsv)\")\n",
        "\n",
        "    def build_index(self, file_name: str):\n",
        "        self.load_file(file_name=file_name)\n",
        "\n",
        "        self.docs['passage'] = self.docs['passage'].apply(self.preprocess_text)\n",
        "\n",
        "        total_tokens = 0\n",
        "        for index, row in self.docs.iterrows():\n",
        "            doc_id, tokens = row['pid'], row['passage']\n",
        "            total_tokens += len(tokens)\n",
        "            for term in tokens:\n",
        "                self.index[term][doc_id] = self.index[term].get(doc_id, 0) + 1\n",
        "                if term not in self.cf:\n",
        "                    self.cf[term] = 1\n",
        "                else:\n",
        "                    self.cf[term] += 1\n",
        "\n",
        "\n",
        "            self.doc_lengths[doc_id] = len(tokens)\n",
        "\n",
        "        self.avgdl = total_tokens / len(self.docs)\n",
        "        self.compute_idf()\n",
        "\n",
        "        corpus = [' '.join(i) for i in list(self.docs['passage'])]\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.embeddings = self.vectorizer.fit_transform(corpus)\n",
        "\n",
        "        self.pid_list = list(self.docs['pid'])\n",
        "        # print(self.docs['passage'][0])\n",
        "        # tokenized_corpus = [doc.split() for doc in self.docs['passage']]\n",
        "\n",
        "        # Create a BM25 instance\n",
        "        self.bm25Okapi = BM25Okapi(self.docs['passage'])\n",
        "\n",
        "    def compute_idf(self):\n",
        "        total_docs = len(self.docs)\n",
        "        for term in self.index:\n",
        "            doc_freq = len(self.index[term])\n",
        "            self.idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
        "\n",
        "    def save_index(self, file_name: str):\n",
        "        with gzip.open(file_name, 'wb', compresslevel=9) as file:\n",
        "            pickle.dump({'index': self.index, 'doc_lengths': self.doc_lengths, 'avgdl': self.avgdl, 'idf': self.idf, 'cf': self.cf, 'vectorizer': self.vectorizer, \"embeddings\": self.embeddings}, file)\n",
        "\n",
        "    def load_index(self, file_name: str):\n",
        "        with gzip.open(file_name, 'rb') as file:\n",
        "            data = pickle.load(file)\n",
        "            self.index = data['index']\n",
        "            self.doc_lengths = data['doc_lengths']\n",
        "            self.avgdl = data['avgdl']\n",
        "            self.idf = data['idf']\n",
        "            self.cf = data['cf']\n",
        "            self.embeddings = data['embeddings']\n",
        "            self.vectorizer = data['vectorizer']\n",
        "\n",
        "\n",
        "class RetrievalModel:\n",
        "    def __init__(self, index: Index) -> None:\n",
        "        self.index = index\n",
        "        self.len_C = len(self.index.index)\n",
        "\n",
        "    def preselect_docs(self, query, min_selected_docs=2):\n",
        "        query_terms = set(query)\n",
        "\n",
        "        relevant_docs = set()\n",
        "\n",
        "        for term in query_terms:\n",
        "            relevant_docs.update(self.index.index.get(term, {}).keys())\n",
        "\n",
        "        return relevant_docs\n",
        "\n",
        "\n",
        "    def query_likelihood(self, query, lambd):\n",
        "        scores = {}\n",
        "\n",
        "        for doc_id in self.preselect_docs(query):\n",
        "            len_doc = self.index.doc_lengths[doc_id]\n",
        "\n",
        "            p_q_Md = 0\n",
        "\n",
        "            for term in query:\n",
        "                df = self.index.index.get(term, {}).get(doc_id, 0)\n",
        "                if len(self.index.index[term]) == 0: continue\n",
        "\n",
        "                cf = self.index.cf[term]\n",
        "                p_q_Md += np.log((1 - lambd) * (df / len_doc) + (lambd * (cf / self.len_C)))\n",
        "\n",
        "            scores[doc_id] = p_q_Md\n",
        "\n",
        "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "        return sorted_scores\n",
        "\n",
        "    def bm25_ranking(self, query, k1 = 1.2, b = 0.75):\n",
        "        scores = {}\n",
        "\n",
        "        for term in query:\n",
        "            score = 0\n",
        "\n",
        "            if len(self.index.index[term]) == 0: continue\n",
        "\n",
        "            idf_value = self.index.idf[term]\n",
        "\n",
        "            for doc_id, tf in self.index.index[term].items():\n",
        "                len_doc = self.index.doc_lengths[doc_id]\n",
        "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
        "                score = idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
        "\n",
        "\n",
        "                if not doc_id in scores:\n",
        "                    scores[doc_id] = score\n",
        "                else:\n",
        "                    scores[doc_id] += score\n",
        "\n",
        "\n",
        "        # sort scores / ranking\n",
        "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "        return sorted_scores\n",
        "\n",
        "\n",
        "    def embeddings_cosign_sim(self, query):\n",
        "        query = [\" \".join(list(query))]\n",
        "        vec_query = self.index.vectorizer.transform(query)\n",
        "\n",
        "        cos = cosine_similarity(self.index.embeddings, vec_query)\n",
        "\n",
        "        scores = dict(zip(self.index.doc_lengths.keys(), cos.flatten()))\n",
        "\n",
        "        non_zero_scores = {k: v for k, v in scores.items() if v != 0}\n",
        "\n",
        "        sorted_scores = {k: v for k, v in sorted(non_zero_scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "        return sorted_scores\n",
        "\n",
        "    def bm25Okapi(self, query):\n",
        "\n",
        "        scores = {}\n",
        "\n",
        "        scores = self.index.bm25Okapi.get_scores(query)\n",
        "\n",
        "        scores = dict(zip(self.index.pid_list, list(scores)))\n",
        "\n",
        "        non_zero_scores = {k: v for k, v in scores.items() if v != 0}\n",
        "\n",
        "        sorted_scores = {k: v for k, v in sorted(non_zero_scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "        return sorted_scores\n",
        "\n",
        "\n",
        "    def bm25_impl_one(self, query):\n",
        "        # Hyperparams to specify\n",
        "        k1 = 1.2\n",
        "        b = 0.75\n",
        "        scores = {}\n",
        "\n",
        "        # Looping through the different docs\n",
        "        # for doc_id, len_doc in self.index.doc_lengths.items():\n",
        "\n",
        "        for doc_id in self.preselect_docs(query):\n",
        "            len_doc = self.index.doc_lengths[doc_id]\n",
        "            if len_doc == 0:\n",
        "                continue\n",
        "            score = 0\n",
        "\n",
        "            # Loop for term in query in the doc\n",
        "            for term in query:\n",
        "\n",
        "                # Calculating/updating the score\n",
        "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
        "                idf_value = self.index.idf.get(term, 0)  # Use 0 as the default value if term is not in idf\n",
        "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
        "\n",
        "\n",
        "\n",
        "            scores[doc_id] = score\n",
        "\n",
        "\n",
        "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "        return sorted_scores\n",
        "\n",
        "    def evaluate_model(self, qrel_file, query_file, lambd=0.5, output_file='evaluation_results.csv'):\n",
        "        # Parse qrel file\n",
        "        qrel_data = pd.read_csv(qrel_file)\n",
        "\n",
        "        # Read query file and preprocess queries\n",
        "        query_data = pd.read_csv(query_file)\n",
        "        query_data['query'] = query_data['query'].apply(self.index.preprocess_text)\n",
        "\n",
        "        # Create a DataFrame to store results\n",
        "        results = pd.DataFrame(columns=['qid','ql_ndcg', 'bm25_ndcg', 'cos_sim_ndcg'])\n",
        "        results['qid'] = query_data['qid']\n",
        "        results['ql_ndcg'] = None\n",
        "        results['bm25_ndcg'] = None\n",
        "        results['cos_sim_ndcg'] = None\n",
        "        results['bm25Okapi'] = None\n",
        "        results['bm25_impl_one'] = None\n",
        "\n",
        "        # query_data = query_data.loc[query_data['qid'] == 722737]\n",
        "        ql_time = 0\n",
        "        bm25_time = 0\n",
        "        bm25_impl_one_time = 0\n",
        "        embeddings_time = 0\n",
        "        bm25Okapi_time = 0\n",
        "        # Evaluate each query\n",
        "        for qid, query in zip(query_data['qid'], query_data['query']):\n",
        "            relevant_docs = qrel_data[(qrel_data['Topic'] == qid) & (qrel_data['Relevancy'] == 1)]['Document#'].tolist()\n",
        "\n",
        "            # Query Likelihood\n",
        "            start_time = time.time()\n",
        "            ql_scores = self.query_likelihood(query, lambd)\n",
        "            end_time = time.time()\n",
        "            ql_time += end_time - start_time\n",
        "            ranked_docs_ql = np.array(list(ql_scores.keys()))\n",
        "            ranked_values_ql = np.array(list(ql_scores.values()))\n",
        "\n",
        "            # Create binary list for relevant and non-relevant documents\n",
        "            y_true_ql = np.isin(ranked_docs_ql, relevant_docs)\n",
        "\n",
        "            # BM25\n",
        "            start_time = time.time()\n",
        "            bm25_scores = self.bm25_ranking(query)\n",
        "            end_time = time.time()\n",
        "            bm25_time += end_time - start_time\n",
        "            ranked_docs_bm25 = np.array(list(bm25_scores.keys()))\n",
        "            ranked_values_bm25 = np.array(list(bm25_scores.values()))\n",
        "\n",
        "            # Create binary list for relevant and non-relevant documents\n",
        "            y_true_bm25 = np.isin(ranked_docs_bm25, relevant_docs)\n",
        "\n",
        "            # Embeddings\n",
        "            start_time = time.time()\n",
        "            cos_sim_scores = self.embeddings_cosign_sim(query)\n",
        "            end_time = time.time()\n",
        "            embeddings_time += end_time - start_time\n",
        "            ranked_docs_cos_sim = np.array(list(cos_sim_scores.keys()))\n",
        "            ranked_values_cos_sim = np.array(list(cos_sim_scores.values()))\n",
        "\n",
        "            # Create binary list for relevant and non-relevant documents\n",
        "            y_true_cos_sim = np.isin(ranked_docs_cos_sim, relevant_docs)\n",
        "\n",
        "\n",
        "            #BM25Okapi\n",
        "            start_time = time.time()\n",
        "            BM25Okapi_scores = self.bm25Okapi(query)\n",
        "            end_time = time.time()\n",
        "            bm25Okapi_time += end_time - start_time\n",
        "            ranked_docs_bm25Okapi = np.array(list(BM25Okapi_scores.keys()))\n",
        "            ranked_values_bm25Okapi = np.array(list(BM25Okapi_scores.values()))\n",
        "\n",
        "\n",
        "\n",
        "            # Create binary list for relevant and non-relevant documents\n",
        "            y_true_bm25Okapi = np.isin(ranked_docs_bm25Okapi, relevant_docs)\n",
        "\n",
        "            #bm25_impl_one\n",
        "            start_time = time.time()\n",
        "            bm25_impl_one_scores = self.bm25_impl_one(query)\n",
        "            end_time = time.time()\n",
        "            bm25_impl_one_time += end_time - start_time\n",
        "            ranked_docs_bm25_impl_one = np.array(list(bm25_impl_one_scores.keys()))\n",
        "            ranked_values_bm25_impl_one = np.array(list(bm25_impl_one_scores.values()))\n",
        "\n",
        "            # Create binary list for relevant and non-relevant documents\n",
        "            y_true_bm25_impl_one = np.isin(ranked_docs_bm25_impl_one, relevant_docs)\n",
        "\n",
        "            if len(y_true_ql) < 2 or len(y_true_bm25) < 2 or len(y_true_cos_sim) < 2: continue\n",
        "\n",
        "            # Calculate NDCG scores\n",
        "            ndcg_ql = ndcg_score([y_true_ql], [ranked_values_ql])\n",
        "            ndcg_bm25 = ndcg_score([y_true_bm25], [ranked_values_bm25])\n",
        "            ndcg_cos_sim = ndcg_score([y_true_cos_sim], [ranked_values_cos_sim])\n",
        "            ndcg_bm25Okapi = ndcg_score([y_true_bm25Okapi], [ranked_values_bm25Okapi])\n",
        "            ndcg_bm25_impl_one = ndcg_score([y_true_bm25_impl_one], [ranked_values_bm25_impl_one])\n",
        "\n",
        "\n",
        "            # Append results to the DataFrame\n",
        "\n",
        "            results.loc[results['qid'] == qid] = [qid, ndcg_ql, ndcg_bm25, ndcg_cos_sim, ndcg_bm25Okapi, ndcg_bm25_impl_one]\n",
        "\n",
        "        print('ql_time: ', ql_time)\n",
        "        print('bm25_time: ', bm25_time)\n",
        "        print('bm25_impl_one_time: ', bm25_impl_one_time)\n",
        "        print('embeddings_time: ', embeddings_time)\n",
        "        print('bm25Okapi_time: ', bm25Okapi_time)\n",
        "        models = ['QL', 'BM25 final implementation', 'BM25 initial implementation', 'Embeddings', 'BM25Okapi']\n",
        "        models_time = [ql_time, bm25_time, bm25_impl_one_time, embeddings_time, bm25Okapi_time]\n",
        "        times_df = pd.DataFrame(\n",
        "        {'Model': models,\n",
        "        'Time': models_time,\n",
        "        })\n",
        "\n",
        "        times_df.to_csv(\"Times_models.csv\")\n",
        "        # Save results to a CSV file\n",
        "        results.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "eYFbBhhW_VQL",
        "outputId": "ded10487-c591-47a6-c392-cb546d06046a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-21b830de37dd>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fine_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-a96415f1bc08>\u001b[0m in \u001b[0;36mbuild_index\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-a96415f1bc08>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tsv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 33548, saw 3\n"
          ]
        }
      ],
      "source": [
        "data_fine_name = \"collection_10.csv\"\n",
        "# data_fine_name = r\"data\\collection.tsv\"\n",
        "index_file_name = 'index.json.gz'\n",
        "\n",
        "index = Index()\n",
        "build = True\n",
        "\n",
        "if build:\n",
        "    index.build_index(data_fine_name)\n",
        "    index.save_index(index_file_name)\n",
        "else:\n",
        "    index.load_index(index_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "3ywfz3dt_VQM",
        "outputId": "6c9489ca-27f2-4a38-c082-6ce3a326ab34"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-ed5b6d657a43>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mretrival_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mretrival_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqrel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-a96415f1bc08>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, qrel_file, query_file, lambd, output_file)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;31m# Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mcos_sim_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_cosign_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0membeddings_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-a96415f1bc08>\u001b[0m in \u001b[0;36membeddings_cosign_sim\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membeddings_cosign_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mvec_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'vectorizer'"
          ]
        }
      ],
      "source": [
        "query_file = \"queries_10.csv\"\n",
        "qrel_file = \"qrel_10.csv\"\n",
        "retrival_model = RetrievalModel(index)\n",
        "\n",
        "retrival_model.evaluate_model(qrel_file, query_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "KrKqO7w6_VQM",
        "outputId": "e492b51d-9a7c-4d71-ab51-6be7ba89fc6c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-6c20f47dbcb6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mquery1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m722737\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "query_file = \"queries_small.csv\"\n",
        "qrel_file = \"qrel_small.csv\"\n",
        "\n",
        "retrival_model = RetrievalModel(index)\n",
        "\n",
        "query_data = pd.read_csv(query_file)\n",
        "query_data['query'] = query_data['query'].apply(Index.preprocess_text)\n",
        "\n",
        "query1 = query_data['query'].iloc[0]\n",
        "query = query_data.loc[query_data['qid'] == 722737, 'query'].iloc[0]\n",
        "\n",
        "start_time = time.time()\n",
        "retrival_model.query_likelihood(query, 0.35)\n",
        "end_time = time.time()\n",
        "print(f\"query_likelihood took {end_time - start_time} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "retrival_model.bm25_ranking(query)\n",
        "end_time = time.time()\n",
        "print(f\"bm25_ranking took {end_time - start_time} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "retrival_model.embeddings_cosign_sim(query)\n",
        "end_time = time.time()\n",
        "print(f\"embeddings_cosign_sim took {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3iKLYOG_VQO"
      },
      "source": [
        "## Test with more efficient retrieval models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbOejT0F_VQP",
        "outputId": "4687c3b5-98c4-4dd5-98f7-09195d013dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{3: 0.7384931496694467, 2: 0.7210401540807048, 1: 0.16786803644225698}\n",
            "{3: 0.7384931496694467, 2: 0.7210401540807048, 1: 0.16786803644225698}\n",
            "{2: -1.6964492894237302, 3: -2.4911848197200785, 1: -2.9656342423967117}\n",
            "{1: -0.8171998292299242, 2: -1.6964492894237302, 3: -2.4911848197200785}\n"
          ]
        }
      ],
      "source": [
        "def preselect_docs(index, query):\n",
        "        query_terms = set(query)\n",
        "\n",
        "        relevant_docs = set()\n",
        "\n",
        "        for term in query_terms:\n",
        "            relevant_docs.update(index.get(term, {}).keys())\n",
        "\n",
        "        return relevant_docs\n",
        "\n",
        "\n",
        "def compute_idf(index, doc_lengths):\n",
        "    idf = {}\n",
        "\n",
        "    total_docs = len(doc_lengths)\n",
        "    for term in index:\n",
        "        doc_freq = len(index[term])\n",
        "        idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
        "    return idf\n",
        "\n",
        "def query_likelihood(index, doc_lengths, query, lambd):\n",
        "    scores = {}\n",
        "\n",
        "    for doc_id in preselect_docs(index, query):\n",
        "        len_C = 12\n",
        "\n",
        "        len_doc = doc_lengths[doc_id]\n",
        "\n",
        "        p_q_Md = 0\n",
        "        for term in query:\n",
        "            df = index.get(term, {}).get(doc_id, 0)\n",
        "            cf = sum(index[term].values())\n",
        "\n",
        "            ts = (1 - lambd) * (df / len_doc) + (lambd * (cf / len_C))\n",
        "\n",
        "            if ts != 0:\n",
        "                p_q_Md += math.log(ts)\n",
        "\n",
        "        scores[doc_id] = p_q_Md\n",
        "\n",
        "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return sorted_scores\n",
        "\n",
        "def query_likelihood_new(index, doc_lengths, query, lambd):\n",
        "    scores = {}\n",
        "    nr_terms = {}\n",
        "\n",
        "    len_C = 12\n",
        "    for term in query:\n",
        "        cf = sum(index[term].values())\n",
        "\n",
        "        for doc_id, df in index[term].items():\n",
        "\n",
        "            doc_len = doc_lengths[doc_id]\n",
        "\n",
        "            ts = (1 - lambd) * (df / doc_len) + (lambd * (cf / len_C))\n",
        "\n",
        "            if not doc_id in scores:\n",
        "                scores[doc_id] = math.log(ts)\n",
        "            else:\n",
        "                scores[doc_id] += math.log(ts)\n",
        "\n",
        "            if not doc_id in nr_terms:\n",
        "                nr_terms[doc_id] =\n",
        "\n",
        "\n",
        "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return sorted_scores\n",
        "\n",
        "    N = len(inverted_index)  # Total number of documents in the collection\n",
        "\n",
        "    doc_scores = np.zeros(len(doc_lengths))\n",
        "    doc_lengths = np.array(list(doc_lengths.values()))\n",
        "\n",
        "    for term in query:\n",
        "        if term in inverted_index:\n",
        "            doc_term_freqs = np.array(list(inverted_index[term].values()))\n",
        "\n",
        "\n",
        "            term_prob = doc_term_freqs / doc_lengths\n",
        "            collection_prob = np.sum(doc_term_freqs) / np.sum(doc_lengths)\n",
        "\n",
        "            doc_scores = doc_scores + np.log((1 - lambda_value) * term_prob + lambda_value * collection_prob)\n",
        "\n",
        "    return doc_scores\n",
        "\n",
        "def bm25_ranking(index, doc_lengths, idf, query):\n",
        "        # Hyperparams to specify\n",
        "        k1 = 1.2\n",
        "        b = 0.75\n",
        "        scores = {}\n",
        "\n",
        "        total_tokens = sum(value for inner_dict in index.values() for value in inner_dict.values())\n",
        "        avgdl = total_tokens / len(doc_lengths)\n",
        "\n",
        "        # Looping through the different docs\n",
        "        for doc_id in preselect_docs(index, query):\n",
        "            len_doc = doc_lengths[doc_id]\n",
        "            if len_doc == 0:\n",
        "                continue\n",
        "            score = 0\n",
        "\n",
        "            # Loop for term in query in the doc\n",
        "            for term in query:\n",
        "                # Calculating/updating the score\n",
        "                tf =  index.get(term, {}).get(doc_id, 0)\n",
        "                idf_value = idf.get(term, 0)  # Use 0 as the default value if term is not in idf\n",
        "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / avgdl)))\n",
        "\n",
        "            scores[doc_id] = score\n",
        "\n",
        "        # sort scores / ranking\n",
        "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "        return sorted_scores\n",
        "\n",
        "def bm25_ranking_new(index, doc_lengths, idf, query):\n",
        "        # Hyperparams to specify\n",
        "        k1 = 1.2\n",
        "        b = 0.75\n",
        "        scores = {}\n",
        "\n",
        "        total_tokens = sum(value for inner_dict in index.values() for value in inner_dict.values())\n",
        "        avgdl = total_tokens / len(doc_lengths)\n",
        "\n",
        "        for term in query:\n",
        "            score = 0\n",
        "            idf_value = idf[term]\n",
        "            for doc_id, tf in index[term].items():\n",
        "                len_doc = doc_lengths[doc_id]\n",
        "\n",
        "                score = idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / avgdl)))\n",
        "\n",
        "                if not doc_id in scores:\n",
        "                    scores[doc_id] = score\n",
        "                else:\n",
        "                    scores[doc_id] += score\n",
        "\n",
        "        # sort scores / ranking\n",
        "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "        return sorted_scores\n",
        "\n",
        "doc_lengths = {1:2, 2:3, 3:7}\n",
        "\n",
        "query = [\"Apple\", \"Phone\"]\n",
        "\n",
        "index = {\"Apple\": {1:1, 2:2, 3:1},\n",
        "         \"Samsung\": {1:1, 3:3},\n",
        "         \"Phone\": {2:1, 3:3}}\n",
        "\n",
        "\n",
        "\n",
        "idf = compute_idf(index, doc_lengths)\n",
        "lambd = 0.35\n",
        "print(bm25_ranking(index, doc_lengths, idf, query))\n",
        "print(bm25_ranking_new(index, doc_lengths, idf, query))\n",
        "\n",
        "print(query_likelihood(index, doc_lengths, query, lambd))\n",
        "print(query_likelihood_new(index, doc_lengths, query, lambd))\n",
        "# jm_smoothed_query_likelihood(index, query, doc_lengths, lambda_value=lambd)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}