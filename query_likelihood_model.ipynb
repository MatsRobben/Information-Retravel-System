{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self) -> None:\n",
    "        self.index = defaultdict(dict)\n",
    "        self.doc_lengths = {}\n",
    "        self.avgdl = 0\n",
    "        self.idf = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        ps = PorterStemmer()\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [ps.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def load_file(self, file_name):\n",
    "        file_extension = file_name.split('.')[-1].lower()\n",
    "\n",
    "        if file_extension == 'csv':\n",
    "            self.docs = pd.read_csv(file_name)\n",
    "        elif file_extension == 'tsv':\n",
    "            self.docs = pd.read_csv(file_name, delimiter='\\t',header=None)\n",
    "            self.docs.columns = ['pid', 'passage']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Supported formats: CSV (.csv) and TSV (.tsv)\")\n",
    "\n",
    "    def build_index(self, file_name: str):\n",
    "        self.load_file(file_name=file_name)\n",
    "\n",
    "        self.docs['passage'] = self.docs['passage'].apply(InvertedIndex.preprocess_text)\n",
    "\n",
    "        total_tokens = 0\n",
    "        for index, row in self.docs.iterrows():\n",
    "            doc_id, tokens = row['pid'], row['passage']\n",
    "            total_tokens += len(tokens)\n",
    "            for term in tokens:\n",
    "                self.index[term][doc_id] = self.index[term].get(doc_id, 0) + 1\n",
    "\n",
    "            self.doc_lengths[doc_id] = len(tokens)\n",
    "\n",
    "        self.avgdl = total_tokens / len(self.docs)\n",
    "        self.compute_idf()\n",
    "\n",
    "    def compute_idf(self):\n",
    "        total_docs = len(self.docs)\n",
    "        for term in self.index:\n",
    "            doc_freq = len(self.index[term])\n",
    "            self.idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
    "\n",
    "    def save_index(self, file_name: str):\n",
    "        with gzip.open(file_name, 'wb', compresslevel=5) as file:\n",
    "            pickle.dump({'index': self.index, 'doc_lengths': self.doc_lengths, 'avgdl': self.avgdl, 'idf': self.idf}, file)\n",
    "\n",
    "    def load_index(self, file_name: str):\n",
    "        with gzip.open(file_name, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            self.index = data['index']\n",
    "            self.doc_lengths = data['doc_lengths']\n",
    "            self.avgdl = data['avgdl']\n",
    "            self.idf = data['idf']\n",
    "\n",
    "\n",
    "class RetrievalModel:\n",
    "    def __init__(self, index: InvertedIndex) -> None:\n",
    "        self.index = index\n",
    "        self.len_C = len(self.index.index)\n",
    "    \n",
    "    def preselect_docs(self, query, min_selected_docs=2):\n",
    "        query_terms = set(query)\n",
    "\n",
    "        relevant_docs = set()\n",
    "\n",
    "        for term in query_terms:\n",
    "            relevant_docs.update(self.index.index.get(term, {}).keys())\n",
    "\n",
    "        while len(relevant_docs) < min_selected_docs:\n",
    "            all_docs = set(self.index.doc_lengths.keys())\n",
    "            additional_doc = random.choice(list(all_docs))\n",
    "            relevant_docs.add(additional_doc)\n",
    "        \n",
    "        return relevant_docs\n",
    "\n",
    "    def query_likelihood(self, query, lambd):\n",
    "        scores = {}\n",
    "        \n",
    "        # for doc_id, len_doc in self.index.doc_lengths.items():\n",
    "        for doc_id in self.preselect_docs(query):\n",
    "            len_doc = self.index.doc_lengths[doc_id]\n",
    "            if len_doc == 0:\n",
    "                continue\n",
    "            p_q_Md = 0\n",
    "            for term in query:\n",
    "                df = self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                cf = sum(self.index.index[term].values())\n",
    "                \n",
    "                ts = (1 - lambd) * (df / len_doc) + (lambd * (cf / self.len_C))\n",
    "                if ts != 0:\n",
    "                    p_q_Md += math.log(ts)\n",
    "\n",
    "            scores[doc_id] = p_q_Md\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "    def bm25_ranking(self, query):\n",
    "        # Hyperparams to specify\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        scores = {}\n",
    "\n",
    "        # Looping through the different docs\n",
    "        # for doc_id, len_doc in self.index.doc_lengths.items():\n",
    "        for doc_id in self.preselect_docs(query):\n",
    "            len_doc = self.index.doc_lengths[doc_id]\n",
    "            if len_doc == 0:\n",
    "                continue\n",
    "            score = 0\n",
    "            \n",
    "            # Loop for term in query in the doc\n",
    "            for term in query:\n",
    "                # Calculating/updating the score\n",
    "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                idf_value = self.index.idf.get(term, 0)  # Use 0 as the default value if term is not in idf\n",
    "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
    "\n",
    "            scores[doc_id] = score \n",
    "        \n",
    "        # sort scores / ranking\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "    def evaluate_model(self, qrel_file, query_file, lambd=0.5, output_file='evaluation_results.csv'):\n",
    "        # Parse qrel file\n",
    "        qrel_data = pd.read_csv(qrel_file)\n",
    "\n",
    "        # Read query file and preprocess queries\n",
    "        query_data = pd.read_csv(query_file)\n",
    "        query_data['query'] = query_data['query'].apply(self.index.preprocess_text)\n",
    "\n",
    "        # Create a DataFrame to store results\n",
    "        results = pd.DataFrame(columns=['qid','ql_ndcg', 'bm25_ndcg'])\n",
    "        results['qid'] = query_data['qid']\n",
    "        results['ql_ndcg'] = None\n",
    "        results['bm25_ndcg'] = None\n",
    "\n",
    "        # Evaluate each query\n",
    "        for qid, query in zip(query_data['qid'], query_data['query']):\n",
    "            relevant_docs = qrel_data[(qrel_data['Topic'] == qid) & (qrel_data['Relevancy'] == 1)]['Document#'].tolist()\n",
    "\n",
    "            # Query Likelihood\n",
    "            ql_scores = self.query_likelihood(query, lambd)\n",
    "            ranked_docs_ql = np.array(list(ql_scores.keys()))\n",
    "            ranked_values_ql = np.array(list(ql_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_ql = np.isin(ranked_docs_ql, relevant_docs)\n",
    "\n",
    "            # BM25\n",
    "            bm25_scores = self.bm25_ranking(query)\n",
    "            ranked_docs_bm25 = np.array(list(bm25_scores.keys()))\n",
    "            ranked_values_bm25 = np.array(list(bm25_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_bm25 = np.isin(ranked_docs_bm25, relevant_docs)\n",
    "\n",
    "            # Calculate NDCG scores\n",
    "            ndcg_ql = ndcg_score([y_true_ql], [ranked_values_ql])\n",
    "            ndcg_bm25 = ndcg_score([y_true_bm25], [ranked_values_bm25])\n",
    "\n",
    "            # print(\"ql:\", ndcg_ql, \"rank\", np.where(ranked_docs_ql == relevant_docs))\n",
    "            # print(\"bm25:\", ndcg_bm25, \"rank\", np.where(ranked_docs_bm25 == relevant_docs))\n",
    "\n",
    "            # Append results to the DataFrame\n",
    "            results.loc[results['qid'] == qid] = [qid, ndcg_ql, ndcg_bm25]\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        results.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid         int64\n",
      "passage    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_fine_name = r\"MSMARCO_SMALL\\collection_small.csv\"\n",
    "# data_fine_name = r\"data\\collection.tsv\"\n",
    "index_file_name = 'index2.json.gz'\n",
    "\n",
    "index = InvertedIndex()\n",
    "build = True\n",
    "\n",
    "if build:\n",
    "    index.build_index(data_fine_name)\n",
    "    index.save_index(index_file_name)\n",
    "else:\n",
    "    index.load_index(index_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "retrival_model.evaluate_model(qrel_file, query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "retrival_model.evaluate_model(qrel_file, query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['execution']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "query_data = pd.read_csv(query_file)\n",
    "query_data['query'] = query_data['query'].apply(index.preprocess_text)\n",
    "\n",
    "query = query_data['query'].iloc[1]\n",
    "\n",
    "print(query)\n",
    "\n",
    "len(retrival_model.preselect_docs(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
