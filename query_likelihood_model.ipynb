{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "class Index:\n",
    "    def __init__(self) -> None:\n",
    "        self.index = defaultdict(dict)\n",
    "        self.embeddings = None\n",
    "        self.doc_lengths = {}\n",
    "        self.avgdl = 0\n",
    "        self.idf = {}\n",
    "        self.cf = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        ps = PorterStemmer()\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [ps.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def load_file(self, file_name):\n",
    "        file_extension = file_name.split('.')[-1].lower()\n",
    "\n",
    "        if file_extension == 'csv':\n",
    "            self.docs = pd.read_csv(file_name)\n",
    "        elif file_extension == 'tsv':\n",
    "            self.docs = pd.read_csv(file_name, delimiter='\\t',header=None)\n",
    "            self.docs.columns = ['pid', 'passage']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Supported formats: CSV (.csv) and TSV (.tsv)\")\n",
    "\n",
    "    def build_index(self, file_name: str):\n",
    "        self.load_file(file_name=file_name)\n",
    "\n",
    "        self.docs['passage'] = self.docs['passage'].apply(self.preprocess_text)\n",
    "\n",
    "        total_tokens = 0\n",
    "        for index, row in self.docs.iterrows():\n",
    "            doc_id, tokens = row['pid'], row['passage']\n",
    "            total_tokens += len(tokens)\n",
    "            for term in tokens:\n",
    "                self.index[term][doc_id] = self.index[term].get(doc_id, 0) + 1\n",
    "                if term not in self.cf:\n",
    "                    self.cf[term] = 1\n",
    "                else:\n",
    "                    self.cf[term] += 1\n",
    "                \n",
    "\n",
    "            self.doc_lengths[doc_id] = len(tokens)\n",
    "\n",
    "        self.avgdl = total_tokens / len(self.docs)\n",
    "        self.compute_idf()\n",
    "\n",
    "        corpus = [' '.join(i) for i in list(self.docs['passage'])]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.embeddings = self.vectorizer.fit_transform(corpus)\n",
    "\n",
    "    def compute_idf(self):\n",
    "        total_docs = len(self.docs)\n",
    "        for term in self.index:\n",
    "            doc_freq = len(self.index[term])\n",
    "            self.idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
    "\n",
    "    def save_index(self, file_name: str):\n",
    "        with gzip.open(file_name, 'wb', compresslevel=9) as file:\n",
    "            pickle.dump({'index': self.index, 'doc_lengths': self.doc_lengths, 'avgdl': self.avgdl, 'idf': self.idf, 'cf': self.cf, 'vectorizer': self.vectorizer, \"embeddings\": self.embeddings}, file)\n",
    "\n",
    "    def load_index(self, file_name: str):\n",
    "        with gzip.open(file_name, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            self.index = data['index']\n",
    "            self.doc_lengths = data['doc_lengths']\n",
    "            self.avgdl = data['avgdl']\n",
    "            self.idf = data['idf']\n",
    "            self.cf = data['cf']\n",
    "            self.embeddings = data['embeddings']\n",
    "            self.vectorizer = data['vectorizer']\n",
    "\n",
    "\n",
    "class RetrievalModel:\n",
    "    def __init__(self, index: Index) -> None:\n",
    "        self.index = index\n",
    "        self.len_C = len(self.index.index)\n",
    "    \n",
    "    def preselect_docs(self, query, min_selected_docs=2):\n",
    "        query_terms = set(query)\n",
    "\n",
    "        relevant_docs = set()\n",
    "\n",
    "        for term in query_terms:\n",
    "            relevant_docs.update(self.index.index.get(term, {}).keys())\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "\n",
    "    def query_likelihood(self, query, lambd):\n",
    "        scores = {}\n",
    "        \n",
    "        for doc_id in self.preselect_docs(query):\n",
    "            len_doc = self.index.doc_lengths[doc_id]\n",
    "\n",
    "            p_q_Md = 0\n",
    "\n",
    "            for term in query:\n",
    "                df = self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                if len(self.index.index[term]) == 0: continue\n",
    "\n",
    "                cf = self.index.cf[term]\n",
    "                p_q_Md += np.log((1 - lambd) * (df / len_doc) + (lambd * (cf / self.len_C)))\n",
    "\n",
    "            scores[doc_id] = p_q_Md\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "    \n",
    "    def bm25_ranking(self, query, k1 = 1.2, b = 0.75):\n",
    "        scores = {}\n",
    "\n",
    "        for term in query:\n",
    "            score = 0\n",
    "\n",
    "            if len(self.index.index[term]) == 0: continue\n",
    "            \n",
    "            idf_value = self.index.idf[term]\n",
    "            \n",
    "            for doc_id, tf in self.index.index[term].items():\n",
    "                len_doc = self.index.doc_lengths[doc_id]\n",
    "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
    "\n",
    "                if not doc_id in scores:\n",
    "                    scores[doc_id] = score\n",
    "                else:\n",
    "                    scores[doc_id] += score\n",
    "        \n",
    "        # sort scores / ranking\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "    def embeddings_cosign_sim(self, query):\n",
    "        query = [\" \".join(list(query))]\n",
    "        vec_query = self.index.vectorizer.transform(query)\n",
    "        \n",
    "        cos = cosine_similarity(self.index.embeddings, vec_query)\n",
    "\n",
    "        scores = dict(zip(self.index.doc_lengths.keys(), cos.flatten()))\n",
    "        \n",
    "        non_zero_scores = {k: v for k, v in scores.items() if v != 0}\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(non_zero_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_scores\n",
    "        \n",
    "    def evaluate_model(self, qrel_file, query_file, lambd=0.5, output_file='evaluation_results.csv'):\n",
    "        # Parse qrel file\n",
    "        qrel_data = pd.read_csv(qrel_file)\n",
    "\n",
    "        # Read query file and preprocess queries\n",
    "        query_data = pd.read_csv(query_file)\n",
    "        query_data['query'] = query_data['query'].apply(self.index.preprocess_text)\n",
    "\n",
    "        # Create a DataFrame to store results\n",
    "        results = pd.DataFrame(columns=['qid','ql_ndcg', 'bm25_ndcg', 'cos_sim_ndcg'])\n",
    "        results['qid'] = query_data['qid']\n",
    "        results['ql_ndcg'] = None\n",
    "        results['bm25_ndcg'] = None\n",
    "        results['cos_sim_ndcg'] = None\n",
    "\n",
    "        # query_data = query_data.loc[query_data['qid'] == 722737]\n",
    "\n",
    "        # Evaluate each query\n",
    "        for qid, query in zip(query_data['qid'], query_data['query']):\n",
    "            relevant_docs = qrel_data[(qrel_data['Topic'] == qid) & (qrel_data['Relevancy'] == 1)]['Document#'].tolist()\n",
    "\n",
    "            # Query Likelihood\n",
    "            ql_scores = self.query_likelihood(query, lambd)\n",
    "            ranked_docs_ql = np.array(list(ql_scores.keys()))\n",
    "            ranked_values_ql = np.array(list(ql_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_ql = np.isin(ranked_docs_ql, relevant_docs)\n",
    "\n",
    "            # BM25\n",
    "            bm25_scores = self.bm25_ranking(query)\n",
    "            ranked_docs_bm25 = np.array(list(bm25_scores.keys()))\n",
    "            ranked_values_bm25 = np.array(list(bm25_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_bm25 = np.isin(ranked_docs_bm25, relevant_docs)\n",
    "\n",
    "            # Embeddings\n",
    "            cos_sim_scores = self.embeddings_cosign_sim(query)\n",
    "            ranked_docs_cos_sim = np.array(list(cos_sim_scores.keys()))\n",
    "            ranked_values_cos_sim = np.array(list(cos_sim_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_cos_sim = np.isin(ranked_docs_cos_sim, relevant_docs)\n",
    "\n",
    "            if len(y_true_ql) < 2 or len(y_true_bm25) < 2 or len(y_true_cos_sim) < 2: continue\n",
    "\n",
    "            # Calculate NDCG scores\n",
    "            ndcg_ql = ndcg_score([y_true_ql], [ranked_values_ql])\n",
    "            ndcg_bm25 = ndcg_score([y_true_bm25], [ranked_values_bm25])\n",
    "            ndcg_cos_sim = ndcg_score([y_true_cos_sim], [ranked_values_cos_sim])\n",
    "\n",
    "            # Append results to the DataFrame\n",
    "            results.loc[results['qid'] == qid] = [qid, ndcg_ql, ndcg_bm25, ndcg_cos_sim]\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        results.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fine_name = r\"MSMARCO_SMALL\\collection_small.csv\"\n",
    "# data_fine_name = r\"data\\collection.tsv\"\n",
    "index_file_name = 'index.json.gz'\n",
    "\n",
    "index = Index()\n",
    "build = False\n",
    "\n",
    "if build:\n",
    "    index.build_index(data_fine_name)\n",
    "    index.save_index(index_file_name)\n",
    "else:\n",
    "    index.load_index(index_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "retrival_model.evaluate_model(qrel_file, query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_likelihood took 0.0 seconds\n",
      "bm25_ranking took 0.0 seconds\n",
      "embeddings_cosign_sim took 0.04980301856994629 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "query_data = pd.read_csv(query_file)\n",
    "query_data['query'] = query_data['query'].apply(Index.preprocess_text)\n",
    "\n",
    "query1 = query_data['query'].iloc[0]\n",
    "query = query_data.loc[query_data['qid'] == 722737, 'query'].iloc[0]\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.query_likelihood(query, 0.35)\n",
    "end_time = time.time()\n",
    "print(f\"query_likelihood took {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.bm25_ranking(query)\n",
    "end_time = time.time()\n",
    "print(f\"bm25_ranking took {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.embeddings_cosign_sim(query)\n",
    "end_time = time.time()\n",
    "print(f\"embeddings_cosign_sim took {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with more efficient retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 0.7384931496694467, 2: 0.7210401540807048, 1: 0.16786803644225698}\n",
      "{3: 0.7384931496694467, 2: 0.7210401540807048, 1: 0.16786803644225698}\n",
      "{2: -1.6964492894237302, 3: -2.4911848197200785, 1: -2.9656342423967117}\n",
      "{1: -0.8171998292299242, 2: -1.6964492894237302, 3: -2.4911848197200785}\n"
     ]
    }
   ],
   "source": [
    "def preselect_docs(index, query):\n",
    "        query_terms = set(query)\n",
    "\n",
    "        relevant_docs = set()\n",
    "\n",
    "        for term in query_terms:\n",
    "            relevant_docs.update(index.get(term, {}).keys())\n",
    "        \n",
    "        return relevant_docs\n",
    "\n",
    "\n",
    "def compute_idf(index, doc_lengths):\n",
    "    idf = {}\n",
    "\n",
    "    total_docs = len(doc_lengths)\n",
    "    for term in index:\n",
    "        doc_freq = len(index[term])\n",
    "        idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
    "    return idf\n",
    "\n",
    "def query_likelihood(index, doc_lengths, query, lambd):\n",
    "    scores = {}\n",
    "    \n",
    "    for doc_id in preselect_docs(index, query):\n",
    "        len_C = 12\n",
    "\n",
    "        len_doc = doc_lengths[doc_id]\n",
    "        \n",
    "        p_q_Md = 0\n",
    "        for term in query:\n",
    "            df = index.get(term, {}).get(doc_id, 0)\n",
    "            cf = sum(index[term].values())\n",
    "            \n",
    "            ts = (1 - lambd) * (df / len_doc) + (lambd * (cf / len_C))\n",
    "\n",
    "            if ts != 0:\n",
    "                p_q_Md += math.log(ts)\n",
    "\n",
    "        scores[doc_id] = p_q_Md\n",
    "\n",
    "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_scores\n",
    "\n",
    "def query_likelihood_new(index, doc_lengths, query, lambd):\n",
    "    scores = {}\n",
    "    nr_terms = {}\n",
    "\n",
    "    len_C = 12\n",
    "    for term in query:\n",
    "        cf = sum(index[term].values())\n",
    "\n",
    "        for doc_id, df in index[term].items():\n",
    "            \n",
    "            doc_len = doc_lengths[doc_id]\n",
    "\n",
    "            ts = (1 - lambd) * (df / doc_len) + (lambd * (cf / len_C))\n",
    "\n",
    "            if not doc_id in scores:\n",
    "                scores[doc_id] = math.log(ts)\n",
    "            else:\n",
    "                scores[doc_id] += math.log(ts)\n",
    "\n",
    "            if not doc_id in nr_terms:\n",
    "                nr_terms[doc_id] = \n",
    "\n",
    "\n",
    "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_scores\n",
    "\n",
    "    N = len(inverted_index)  # Total number of documents in the collection\n",
    "\n",
    "    doc_scores = np.zeros(len(doc_lengths))\n",
    "    doc_lengths = np.array(list(doc_lengths.values()))\n",
    "\n",
    "    for term in query:\n",
    "        if term in inverted_index:\n",
    "            doc_term_freqs = np.array(list(inverted_index[term].values()))\n",
    "\n",
    "\n",
    "            term_prob = doc_term_freqs / doc_lengths\n",
    "            collection_prob = np.sum(doc_term_freqs) / np.sum(doc_lengths)\n",
    "\n",
    "            doc_scores = doc_scores + np.log((1 - lambda_value) * term_prob + lambda_value * collection_prob)\n",
    "\n",
    "    return doc_scores\n",
    "\n",
    "def bm25_ranking(index, doc_lengths, idf, query):\n",
    "        # Hyperparams to specify\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        scores = {}\n",
    "\n",
    "        total_tokens = sum(value for inner_dict in index.values() for value in inner_dict.values())\n",
    "        avgdl = total_tokens / len(doc_lengths)\n",
    "\n",
    "        # Looping through the different docs\n",
    "        for doc_id in preselect_docs(index, query):\n",
    "            len_doc = doc_lengths[doc_id]\n",
    "            if len_doc == 0:\n",
    "                continue\n",
    "            score = 0\n",
    "            \n",
    "            # Loop for term in query in the doc\n",
    "            for term in query:\n",
    "                # Calculating/updating the score\n",
    "                tf =  index.get(term, {}).get(doc_id, 0)\n",
    "                idf_value = idf.get(term, 0)  # Use 0 as the default value if term is not in idf\n",
    "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / avgdl)))\n",
    "\n",
    "            scores[doc_id] = score \n",
    "        \n",
    "        # sort scores / ranking\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "def bm25_ranking_new(index, doc_lengths, idf, query):\n",
    "        # Hyperparams to specify\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        scores = {}\n",
    "\n",
    "        total_tokens = sum(value for inner_dict in index.values() for value in inner_dict.values())\n",
    "        avgdl = total_tokens / len(doc_lengths)\n",
    "\n",
    "        for term in query:\n",
    "            score = 0\n",
    "            idf_value = idf[term]\n",
    "            for doc_id, tf in index[term].items():\n",
    "                len_doc = doc_lengths[doc_id]\n",
    "                \n",
    "                score = idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / avgdl)))\n",
    "\n",
    "                if not doc_id in scores:\n",
    "                    scores[doc_id] = score\n",
    "                else:\n",
    "                    scores[doc_id] += score\n",
    "        \n",
    "        # sort scores / ranking\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "doc_lengths = {1:2, 2:3, 3:7}\n",
    "\n",
    "query = [\"Apple\", \"Phone\"]\n",
    "\n",
    "index = {\"Apple\": {1:1, 2:2, 3:1},\n",
    "         \"Samsung\": {1:1, 3:3},\n",
    "         \"Phone\": {2:1, 3:3}}\n",
    "\n",
    "\n",
    "\n",
    "idf = compute_idf(index, doc_lengths)\n",
    "lambd = 0.35\n",
    "print(bm25_ranking(index, doc_lengths, idf, query))\n",
    "print(bm25_ranking_new(index, doc_lengths, idf, query))\n",
    "\n",
    "print(query_likelihood(index, doc_lengths, query, lambd))\n",
    "print(query_likelihood_new(index, doc_lengths, query, lambd))\n",
    "# jm_smoothed_query_likelihood(index, query, doc_lengths, lambda_value=lambd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
