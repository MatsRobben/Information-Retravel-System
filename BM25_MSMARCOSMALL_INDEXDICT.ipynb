{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "khje3GXF3EY5"
   },
   "outputs": [],
   "source": [
    "# basic imports needed\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vNhZmojNCAva"
   },
   "outputs": [],
   "source": [
    "def create_index(documents):\n",
    "    # Creating a dict to store the term freqs\n",
    "    index = defaultdict(dict)\n",
    "\n",
    "    # Creating a dict to store the document lengths\n",
    "    doc_len = {}\n",
    "\n",
    "    # Creating a set to store the unique terms\n",
    "    terms = set()\n",
    "\n",
    "    # Looping over the documents\n",
    "    for i, document in enumerate(documents):\n",
    "\n",
    "        # Looping over the terms in the document\n",
    "        for term in document:\n",
    "\n",
    "            # Change the term frequency\n",
    "            index[term][i] = index[term].get(i, 0) + 1\n",
    "\n",
    "            # Add the term to the set\n",
    "            terms.add(term)\n",
    "\n",
    "        # Store the length of the document\n",
    "        doc_len[i] = len(document)\n",
    "\n",
    "    # Compute the average document length\n",
    "    avgdl = sum(doc_len.values()) / len(doc_len)\n",
    "\n",
    "    # Compute the idf for each term\n",
    "    idf = {}\n",
    "\n",
    "    #print(terms)\n",
    "\n",
    "    for term in terms:\n",
    "        df = len(index[term])\n",
    "        idf[term] = math.log(len(documents)/df)\n",
    "\n",
    "\n",
    "    return index, doc_len, avgdl, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzK2tYavCC_e",
    "outputId": "60f25ad1-916d-42bc-dc8f-843fbdc16828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home / medterms medical dictionary a-z list / intelligence quotient definition Medical Definition of Intelligence quotient Intelligence quotient: An attempt to measure the intelligence of someone. Abbreviated IQ. The IQ score is usually based upon the results of a written test.\n"
     ]
    }
   ],
   "source": [
    "# MSMARCO SMALL dataset made by the code MSMARCO_SMALL.py\n",
    "\n",
    "# Here we get the corpus, split the words and make a list of lists containing the passages.\n",
    "\n",
    "collection = pd.DataFrame(pd.read_csv(r\"C:\\Users\\Raul\\Documents\\Information-Retravel-System\\MSMARCO_SMALL\\collection_small.csv\"))\n",
    "\n",
    "print(collection['passage'][8770])\n",
    "\n",
    "collection['passage'] = collection['passage'].str.split()\n",
    "\n",
    "#print(collection)\n",
    "\n",
    "list_of_lists_collection = collection['passage'].tolist()\n",
    "\n",
    "list_of_lists_collection_pid = collection['pid'].tolist()\n",
    "\n",
    "#print(list_of_lists)\n",
    "\n",
    "tf = []\n",
    "\n",
    "for doc in list_of_lists_collection:\n",
    "  tf.append(dict(Counter(doc)))\n",
    "\n",
    "index, doc_len, avgdl, idf = create_index(list_of_lists_collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pk6LaYUCETa",
    "outputId": "afc8f449-d747-4915-faca-d633af189e37"
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "# print(f\"Index: {index}\")\n",
    "# print(f\"Document Lengths: {doc_len}\")\n",
    "# print(f\"Average Document Length: {avgdl}\")\n",
    "# print(f\"Term Frequencies per doc in order: {tf}\")\n",
    "# print(f\"Inverse Document Frequencies: {idf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg6RMX71zS6D"
   },
   "source": [
    "# TF percentages rather than numbers, might be useful at some point. NOT USED NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VB7SH9BIwpX",
    "outputId": "e627c569-c865-49d0-b7a7-dc128d6f833e"
   },
   "outputs": [],
   "source": [
    "def tf_perc(iindex):\n",
    "    tf = []\n",
    "    for doc in iindex.values():\n",
    "        term_freq = {}\n",
    "        total_words = sum(doc.values())\n",
    "        for term in doc:\n",
    "            term_freq[term] = doc[term] / total_words\n",
    "        tf.append(term_freq)\n",
    "    return tf\n",
    "\n",
    "# Example usage\n",
    "tf2 = tf_perc(index)\n",
    "# print(tf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-02g2fbDG7R"
   },
   "source": [
    "Code below is inspired by: https://medium.com/@evertongomede/understanding-the-bm25-ranking-algorithm-19f6d45c6ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LZglDHf7WE_",
    "outputId": "5de8b6d1-e866-4278-c1a7-06188a5733e5"
   },
   "outputs": [],
   "source": [
    "#BM25 ranking algorithm\n",
    "\n",
    "def bm25_ranking(tf, idf, docs, query):\n",
    "    # Hyperparams to specify\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    scores = {}\n",
    "\n",
    "    # Looping through the different docs\n",
    "    for doc_nr in range(len(docs)):\n",
    "        score = 0\n",
    "        doc = docs[doc_nr]\n",
    "        \n",
    "        # Loop for term in query in the doc\n",
    "        for term in query:\n",
    "            if str(term) in list(tf[doc_nr].keys()):\n",
    "\n",
    "                # Calculating/updating the score\n",
    "                score += idf[term] * ((tf[doc_nr][term] * (k1 + 1)) / (tf[doc_nr][term] + k1 * (1 - b + b * (len(doc) / avgdl))))\n",
    "\n",
    "        scores[list_of_lists_collection_pid[doc_nr]] = score \n",
    "    \n",
    "    # sort scores / ranking\n",
    "    # print(scores)\n",
    "    sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_scores\n",
    "\n",
    "# Use on MSMARCO\n",
    "\n",
    "# using the queries and the corpus from earlier on, qrel file only used for eval later on.\n",
    "\n",
    "queries = pd.DataFrame(pd.read_csv(r\"C:\\Users\\Raul\\Documents\\Information-Retravel-System\\MSMARCO_SMALL\\queries_small.csv\"))\n",
    "\n",
    "#print(queries.head(5))\n",
    "\n",
    "queries['query'] = queries['query'].str.split()\n",
    "\n",
    "list_of_lists_query = queries['query'].tolist()\n",
    "\n",
    "list_of_qids = queries['qid'].tolist()\n",
    "\n",
    "#print(list_of_lists_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LZglDHf7WE_",
    "outputId": "5de8b6d1-e866-4278-c1a7-06188a5733e5"
   },
   "outputs": [],
   "source": [
    "qrel = pd.DataFrame(pd.read_csv(r\"C:\\Users\\Raul\\Documents\\Information-Retravel-System\\MSMARCO_SMALL\\qrel_small.csv\"))\n",
    "\n",
    "scores = []\n",
    "count = 0\n",
    "eval_lists = []\n",
    "for query in list_of_lists_query:\n",
    "  #print(count)\n",
    "  scores.append([list_of_qids[count], bm25_ranking(tf, idf, list_of_lists_collection, query)])\n",
    "  \n",
    "  count += 1\n",
    "    \n",
    "\n",
    "# print(scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(qrel.iloc[0, 3].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'col1': ['x', 'y', 'z', 'x'], 'col2': ['y', 'y', 'y', 'z']})\n",
    "\n",
    "print(df.groupby('col1')['col2'].apply(lambda x: (x == 'y').all()).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 90.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1049079, 0, 8756089, 1, 17.03425836985787, 'RUN1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "eval_list = []\n",
    "\n",
    "for query, docs in tqdm(scores):\n",
    "    count = 0\n",
    "    for doc, score in docs.items():\n",
    "        count+=1\n",
    "        if count > 20:\n",
    "            break\n",
    "        topic_doc_pair = (np.int64(query), np.int64(doc))\n",
    "\n",
    "        if ((qrel['Topic'] == topic_doc_pair[0]) & (qrel['Document#'] == topic_doc_pair[1])).any():\n",
    "        \n",
    "            rel = 1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            rel = 0\n",
    "            \n",
    "        eval_list.append([query, rel, doc, list(docs.keys()).index(doc)+1, score, 'RUN1'])\n",
    "        \n",
    "print(eval_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1049079, 1, 7833823, 2, 16.901728523361186, 'RUN1']\n",
      "[1049079, 0, 981344, 3, 11.869049631462186, 'RUN1']\n",
      "[1049079, 0, 6591016, 4, 10.268837785824317, 'RUN1']\n",
      "[1049079, 0, 2813709, 5, 9.980594316345698, 'RUN1']\n"
     ]
    }
   ],
   "source": [
    "print(eval_list[1])\n",
    "\n",
    "print(eval_list[2])\n",
    "\n",
    "print(eval_list[3])\n",
    "\n",
    "print(eval_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(file_path, data):\n",
    "    with open(file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for sublist in data:\n",
    "            writer.writerow(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "write_to_csv('BM25_evalcsv.csv', eval_list)\n",
    "# with open('BM25_eval.txt', 'w') as f:\n",
    "#     for sublist in eval_list:\n",
    "#         line = ' '.join([str(elem) for elem in sublist])\n",
    "#         f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUGEZz9tnRUN"
   },
   "outputs": [],
   "source": [
    "# from relevancy import relevancy_lookup\n",
    "# import csv\n",
    "\n",
    "# def process_qrel_file(qrel_path):\n",
    "#     relevancies = relevancy_lookup()\n",
    "\n",
    "#     with open(qrel_path) as file:\n",
    "#         qrel_file = csv.reader(file, delimiter=\"\\t\")\n",
    "#         for line in qrel_file:\n",
    "#             query, document, relevancy = parse_qrel_line(line)\n",
    "#             relevancies.add(query, document, relevancy)\n",
    "#     return relevancies\n",
    "\n",
    "# def parse_qrel_line(line):\n",
    "#     #query_id, _, document_id, relevance\n",
    "#     line = line[0].split()\n",
    "#     return int(line[0]), line[2], int(line[3])\n",
    "\n",
    "# qrel_path = \"msmarco-docdev-qrels.tsv\"\n",
    "# relevancies = process_qrel_file(qrel_path)\n",
    "# print(relevancies.relevancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOzG2vtWM3S9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
