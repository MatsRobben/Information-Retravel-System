{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzipx\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from pympler import asizeof\n",
    "\n",
    "class Index:\n",
    "    def __init__(self) -> None:\n",
    "        self.index = defaultdict(dict)\n",
    "        self.embeddings = None\n",
    "        self.bm25Okapi = None\n",
    "        self.doc_lengths = {}\n",
    "        self.avgdl = 0\n",
    "        self.idf = {}\n",
    "        self.cf = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        ps = PorterStemmer()\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [ps.stem(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def load_file(self, file_name):\n",
    "        file_extension = file_name.split('.')[-1].lower()\n",
    "\n",
    "        if file_extension == 'csv':\n",
    "            self.docs = pd.read_csv(file_name)\n",
    "        elif file_extension == 'tsv':\n",
    "            self.docs = pd.read_csv(file_name, delimiter='\\t',header=None)\n",
    "            self.docs.columns = ['pid', 'passage']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Supported formats: CSV (.csv) and TSV (.tsv)\")\n",
    "\n",
    "    def build_index(self, file_name: str):\n",
    "        times = []\n",
    "\n",
    "        self.load_file(file_name=file_name)\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.docs['passage'] = self.docs['passage'].apply(self.preprocess_text)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        total_tokens = 0\n",
    "        for index, row in self.docs.iterrows():\n",
    "            doc_id, tokens = row['pid'], row['passage']\n",
    "            total_tokens += len(tokens)\n",
    "            for term in tokens:\n",
    "                self.index[term][doc_id] = self.index[term].get(doc_id, 0) + 1\n",
    "                if term not in self.cf:\n",
    "                    self.cf[term] = 1\n",
    "                else:\n",
    "                    self.cf[term] += 1\n",
    "\n",
    "\n",
    "            self.doc_lengths[doc_id] = len(tokens)\n",
    "\n",
    "        self.avgdl = total_tokens / len(self.docs)\n",
    "        self.compute_idf()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        corpus = [' '.join(i) for i in list(self.docs['passage'])]\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.embeddings = self.vectorizer.fit_transform(corpus)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.bm25Okapi = BM25Okapi(self.docs['passage'])\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        return times\n",
    "\n",
    "    def compute_idf(self):\n",
    "        total_docs = len(self.docs)\n",
    "        for term in self.index:\n",
    "            doc_freq = len(self.index[term])\n",
    "            self.idf[term] = np.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
    "\n",
    "    def save_index(self, file_name: str):\n",
    "        with gzip.open(file_name, 'wb', compresslevel=9) as file:\n",
    "            pickle.dump({'index': self.index, \n",
    "                         'doc_lengths': self.doc_lengths, \n",
    "                         'avgdl': self.avgdl, \n",
    "                         'idf': self.idf, \n",
    "                         'cf': self.cf, \n",
    "                         'vectorizer': self.vectorizer, \n",
    "                         'embeddings': self.embeddings,\n",
    "                         'bm25Okapi': self.bm25Okapi}, file)\n",
    "\n",
    "    def load_index(self, file_name: str, get_size=False):\n",
    "        with gzip.open(file_name, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            self.index = data['index']\n",
    "            self.doc_lengths = data['doc_lengths']\n",
    "            self.avgdl = data['avgdl']\n",
    "            self.idf = data['idf']\n",
    "            self.cf = data['cf']\n",
    "            self.embeddings = data['embeddings']\n",
    "            self.vectorizer = data['vectorizer']\n",
    "            self.bm25Okapi = data['bm25Okapi']\n",
    "\n",
    "            if get_size:\n",
    "                return [self.memory_usage(self.index) + self.memory_usage(self.doc_lengths) + self.memory_usage(self.avgdl) + self.memory_usage(self.idf) + self.memory_usage(self.cf),\n",
    "                        self.memory_usage(self.embeddings) + self.memory_usage(self.vectorizer),\n",
    "                        self.memory_usage(self.bm25Okapi)]\n",
    "\n",
    "\n",
    "    def memory_usage(self, obj):\n",
    "        return asizeof.asizeof(obj) / (1024 * 1024)\n",
    "\n",
    "class RetrievalModel:\n",
    "    def __init__(self, index: Index) -> None:\n",
    "        self.index = index\n",
    "        self.len_C = len(self.index.index)\n",
    "\n",
    "    def preselect_docs(self, query):\n",
    "        query_terms = set(query)\n",
    "\n",
    "        relevant_docs = set()\n",
    "\n",
    "        for term in query_terms:\n",
    "            relevant_docs.update(self.index.index.get(term, {}).keys())\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "\n",
    "    def query_likelihood(self, query, lambd=0.35):\n",
    "        scores = {}\n",
    "\n",
    "        for doc_id in self.preselect_docs(query):\n",
    "            len_doc = self.index.doc_lengths[doc_id]\n",
    "\n",
    "            p_q_Md = 0\n",
    "\n",
    "            for term in query:\n",
    "                df = self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                if len(self.index.index[term]) == 0: continue\n",
    "\n",
    "                cf = self.index.cf[term]\n",
    "                p_q_Md += np.log((1 - lambd) * (df / len_doc) + (lambd * (cf / self.len_C)))\n",
    "\n",
    "            scores[doc_id] = p_q_Md\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_scores\n",
    "\n",
    "    def bm25_ranking(self, query, k1 = 1.2, b = 0.75):\n",
    "        scores = {}\n",
    "\n",
    "        for term in query:\n",
    "            score = 0\n",
    "\n",
    "            if len(self.index.index[term]) == 0: continue\n",
    "\n",
    "            idf_value = self.index.idf[term]\n",
    "\n",
    "            for doc_id, tf in self.index.index[term].items():\n",
    "                len_doc = self.index.doc_lengths[doc_id]\n",
    "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                score = idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
    "\n",
    "                if not doc_id in scores:\n",
    "                    scores[doc_id] = score\n",
    "                else:\n",
    "                    scores[doc_id] += score\n",
    "\n",
    "        # sort scores / ranking\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_scores\n",
    "\n",
    "\n",
    "    def embeddings_cosign_sim(self, query):\n",
    "        query = [\" \".join(list(query))]\n",
    "        vec_query = self.index.vectorizer.transform(query)\n",
    "\n",
    "        cos = cosine_similarity(self.index.embeddings, vec_query)\n",
    "\n",
    "        scores = dict(zip(self.index.doc_lengths.keys(), cos.flatten()))\n",
    "\n",
    "        non_zero_scores = {k: v for k, v in scores.items() if v != 0}\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(non_zero_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_scores\n",
    "\n",
    "    def bm25Okapi(self, query):\n",
    "\n",
    "        scores = {}\n",
    "\n",
    "        scores = self.index.bm25Okapi.get_scores(query)\n",
    "\n",
    "        scores = dict(zip(list(self.index.doc_lengths.keys()), list(scores)))\n",
    "\n",
    "        non_zero_scores = {k: v for k, v in scores.items() if v != 0}\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(non_zero_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_scores\n",
    "\n",
    "\n",
    "    def bm25_impl_one(self, query):\n",
    "        # Hyperparams to specify\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        scores = {}\n",
    "\n",
    "        for doc_id in self.preselect_docs(query):\n",
    "            len_doc = self.index.doc_lengths[doc_id]\n",
    "            if len_doc == 0:\n",
    "                continue\n",
    "            score = 0\n",
    "\n",
    "            # Loop for term in query in the doc\n",
    "            for term in query:\n",
    "\n",
    "                # Calculating/updating the score\n",
    "                tf =  self.index.index.get(term, {}).get(doc_id, 0)\n",
    "                idf_value = self.index.idf.get(term, 0)  # Use 0 as the default value if term is not in idf\n",
    "                score += idf_value * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len_doc / self.index.avgdl)))\n",
    "\n",
    "            scores[doc_id] = score\n",
    "\n",
    "\n",
    "        sorted_scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        return sorted_scores\n",
    "\n",
    "    def evaluate_model(self, qrel_file, query_file, lambd=0.5, output_file='evaluation_results.csv'):\n",
    "        # Parse qrel file\n",
    "        qrel_data = pd.read_csv(qrel_file)\n",
    "\n",
    "        # Read query file and preprocess queries\n",
    "        query_data = pd.read_csv(query_file)\n",
    "        query_data['query'] = query_data['query'].apply(self.index.preprocess_text)\n",
    "\n",
    "        # Create a DataFrame to store results\n",
    "        results = pd.DataFrame(columns=['qid','ql_ndcg', 'bm25_ndcg', 'cos_sim_ndcg'])\n",
    "        results['qid'] = query_data['qid']\n",
    "        results['ql_ndcg'] = None\n",
    "        results['bm25_ndcg'] = None\n",
    "        results['cos_sim_ndcg'] = None\n",
    "        results['bm25Okapi'] = None\n",
    "        results['bm25_impl_one'] = None\n",
    "\n",
    "        # query_data = query_data.loc[query_data['qid'] == 722737]\n",
    "        ql_times = []\n",
    "        bm25_times = []\n",
    "        bm25_impl_one_times = []\n",
    "        embeddings_times = []\n",
    "        bm25Okapi_times = []\n",
    "        # Evaluate each query\n",
    "        for qid, query in zip(query_data['qid'], query_data['query']):\n",
    "            relevant_docs = qrel_data[(qrel_data['Topic'] == qid) & (qrel_data['Relevancy'] == 1)]['Document#'].tolist()\n",
    "\n",
    "            # Query Likelihood\n",
    "            start_time = time.time()\n",
    "            ql_scores = self.query_likelihood(query, lambd)\n",
    "            end_time = time.time()\n",
    "            ql_times.append(end_time - start_time)\n",
    "            ranked_docs_ql = np.array(list(ql_scores.keys()))\n",
    "            ranked_values_ql = np.array(list(ql_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_ql = np.isin(ranked_docs_ql, relevant_docs)\n",
    "\n",
    "            # BM25\n",
    "            start_time = time.time()\n",
    "            bm25_scores = self.bm25_ranking(query)\n",
    "            end_time = time.time()\n",
    "            bm25_times.append(end_time - start_time)\n",
    "            ranked_docs_bm25 = np.array(list(bm25_scores.keys()))\n",
    "            ranked_values_bm25 = np.array(list(bm25_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_bm25 = np.isin(ranked_docs_bm25, relevant_docs)\n",
    "\n",
    "            # Embeddings\n",
    "            start_time = time.time()\n",
    "            cos_sim_scores = self.embeddings_cosign_sim(query)\n",
    "            end_time = time.time()\n",
    "            embeddings_times.append(end_time - start_time)\n",
    "            ranked_docs_cos_sim = np.array(list(cos_sim_scores.keys()))\n",
    "            ranked_values_cos_sim = np.array(list(cos_sim_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_cos_sim = np.isin(ranked_docs_cos_sim, relevant_docs)\n",
    "\n",
    "\n",
    "            #BM25Okapi\n",
    "            start_time = time.time()\n",
    "            BM25Okapi_scores = self.bm25Okapi(query)\n",
    "            end_time = time.time()\n",
    "            bm25Okapi_times.append(end_time - start_time)\n",
    "            ranked_docs_bm25Okapi = np.array(list(BM25Okapi_scores.keys()))\n",
    "            ranked_values_bm25Okapi = np.array(list(BM25Okapi_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_bm25Okapi = np.isin(ranked_docs_bm25Okapi, relevant_docs)\n",
    "\n",
    "            #bm25_impl_one\n",
    "            start_time = time.time()\n",
    "            bm25_impl_one_scores = self.bm25_impl_one(query)\n",
    "            end_time = time.time()\n",
    "            bm25_impl_one_times.append(end_time - start_time)\n",
    "            ranked_docs_bm25_impl_one = np.array(list(bm25_impl_one_scores.keys()))\n",
    "            ranked_values_bm25_impl_one = np.array(list(bm25_impl_one_scores.values()))\n",
    "\n",
    "            # Create binary list for relevant and non-relevant documents\n",
    "            y_true_bm25_impl_one = np.isin(ranked_docs_bm25_impl_one, relevant_docs)\n",
    "\n",
    "            if len(y_true_ql) < 2 or len(y_true_bm25) < 2 or len(y_true_cos_sim) < 2: continue\n",
    "\n",
    "            # Calculate NDCG scores\n",
    "            ndcg_ql = ndcg_score([y_true_ql], [ranked_values_ql])\n",
    "            ndcg_bm25 = ndcg_score([y_true_bm25], [ranked_values_bm25])\n",
    "            ndcg_cos_sim = ndcg_score([y_true_cos_sim], [ranked_values_cos_sim])\n",
    "            ndcg_bm25Okapi = ndcg_score([y_true_bm25Okapi], [ranked_values_bm25Okapi])\n",
    "            ndcg_bm25_impl_one = ndcg_score([y_true_bm25_impl_one], [ranked_values_bm25_impl_one])\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # Append results to the DataFrame\n",
    "            results.loc[results['qid'] == qid] = [qid, ndcg_ql, ndcg_bm25, ndcg_cos_sim, ndcg_bm25Okapi, ndcg_bm25_impl_one]\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        results.to_csv(output_file, index=False)\n",
    "\n",
    "        return [np.average(ql_times), np.average(bm25_times), np.average(bm25_impl_one_times), np.average(embeddings_times), np.average(bm25Okapi_times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fine_name = r\"MSMARCO_SMALL\\collection_0.1.csv\"\n",
    "index_file_name = 'index.json.gz'\n",
    "\n",
    "index = Index()\n",
    "build = True\n",
    "\n",
    "if build:\n",
    "    times, index_size = index.build_index(data_fine_name)\n",
    "    index.save_index(index_file_name)\n",
    "else:\n",
    "    index.load_index(index_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.516738891601562, 0.8827648162841797, 0.2670776844024658, 0.13420748710632324]\n",
      "[125.27284455299377, 8.292277097702026, 2.4589576721191406, 1.216179370880127]\n",
      "[253.64327335357666, 16.65377426147461, 4.974998950958252, 2.3776464462280273]\n",
      "[642.5453038215637, 45.31479740142822, 13.046379089355469, 6.780766487121582]\n",
      "[1298.010850429535, 85.57694458961487, 25.10372543334961, 13.121039867401123]\n",
      "[2502.9415130615234, 175.954283952713, 54.66093707084656, 29.36467409133911]\n"
     ]
    }
   ],
   "source": [
    "output_file = 'index_results.csv'\n",
    "\n",
    "percentages = [0.1, 1, 2, 5, 10, 20]\n",
    "\n",
    "results_index = pd.DataFrame(columns=['percentage_of_col', 'preprocessing_time', 'inverted_index_time', 'vercor_index_time', 'existing_implemention_time'])\n",
    "\n",
    "for percentage in percentages:\n",
    "    data_fine_name = f\"MSMARCO_SMALL\\collection_{percentage}.csv\"\n",
    "    index_file_name = f'index_{percentage}.json.gz'\n",
    "\n",
    "    index = Index()\n",
    "\n",
    "    times = index.build_index(data_fine_name)\n",
    "    index.save_index(index_file_name)\n",
    "    print(times)\n",
    "    results_index.loc[len(results_index.index)] = [percentage] + times\n",
    "\n",
    "results_index.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 [26.539085388183594, 5.99493408203125, 21.3951416015625]\n",
      "1 [186.50894165039062, 34.87218475341797, 200.80287170410156]\n",
      "2 [358.7286834716797, 65.38561248779297, 401.15380096435547]\n",
      "5 [854.5349502563477, 150.8685760498047, 996.8666381835938]\n",
      "10 [1656.4396667480469, 281.41382598876953, 1982.8914031982422]\n",
      "20 [3290.353202819824, 547.0382614135742, 3962.390151977539]\n"
     ]
    }
   ],
   "source": [
    "percentages = [0.1, 1, 2, 5, 10, 20]\n",
    "\n",
    "sizes_df = pd.DataFrame(columns=['percentage_of_col', \n",
    "                                 'inverted_index', \n",
    "                                 'TF_IDF_embeddings',  \n",
    "                                 'BM25Okapi'])\n",
    "\n",
    "for percentage in percentages:\n",
    "    index_file_name = f'index_{percentage}.json.gz'\n",
    "    query_file = f\"MSMARCO_SMALL\\queries_{percentage}.csv\"\n",
    "    qrel_file = f\"MSMARCO_SMALL\\qrel_{percentage}.csv\"\n",
    "\n",
    "    index = Index()\n",
    "    size = index.load_index(index_file_name, get_size=True)\n",
    "    print(percentage, size)\n",
    "\n",
    "    sizes_df.loc[len(sizes_df.index)] = [percentage] + size\n",
    "\n",
    "sizes_df.to_csv('sizes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002711301896630264, 0.0004750461113162157, 0.001381862454298066, 0.006189119525072052, 0.013134142247641959]\n",
      "[0.029487557411193847, 0.0059511327743530275, 0.016143603324890135, 0.0498958683013916, 0.14025989532470703]\n",
      "[0.049515140056610105, 0.010955033302307128, 0.027992122173309326, 0.09839768648147583, 0.2809553933143616]\n",
      "[0.13241675853729248, 0.030601599216461182, 0.07611552953720092, 0.24761239290237427, 0.6738634943962097]\n",
      "[0.2259249210357666, 0.06120824098587036, 0.1330464029312134, 0.520922110080719, 1.3213372540473938]\n",
      "[0.5476163530349731, 0.13780436038970947, 0.3136431550979614, 1.0619917941093444, 2.9175866532325743]\n"
     ]
    }
   ],
   "source": [
    "percentages = [0.1, 1, 2, 5, 10, 20]\n",
    "# percentages = [0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "times_df = pd.DataFrame(columns=['percentage_of_col', \n",
    "                                 'QL', \n",
    "                                 'BM25 final implementation', \n",
    "                                 'BM25 initial implementation', \n",
    "                                 'Embeddings', \n",
    "                                 'BM25Okapi'])\n",
    "\n",
    "for percentage in percentages:\n",
    "    index_file_name = f'index_{percentage}.json.gz'\n",
    "    query_file = f\"MSMARCO_SMALL\\queries_{percentage}.csv\"\n",
    "    qrel_file = f\"MSMARCO_SMALL\\qrel_{percentage}.csv\"\n",
    "\n",
    "    index = Index()\n",
    "    index.load_index(index_file_name)\n",
    "\n",
    "    retrival_model = RetrievalModel(index)\n",
    "    model_times = retrival_model.evaluate_model(qrel_file, query_file, output_file=f'evaluation_results_{percentage}.csv')\n",
    "    print(model_times)\n",
    "    times_df.loc[len(times_df.index)] = [percentage] + model_times\n",
    "\n",
    "times_df.to_csv(\"Times_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ql_time:  0.0031063759868795223\n",
      "bm25_time:  0.0004903696703188347\n",
      "bm25_impl_one_time:  0.0017046919374754934\n",
      "embeddings_time:  0.005431151751315955\n",
      "bm25Okapi_time:  0.01197624612938274\n"
     ]
    }
   ],
   "source": [
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "retrival_model.evaluate_model(qrel_file, query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_likelihood took 0.22401165962219238 seconds\n",
      "bm25_ranking took 0.06699109077453613 seconds\n",
      "embeddings_cosign_sim took 0.5279767513275146 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query_file = r\"MSMARCO_SMALL\\queries_small.csv\"\n",
    "qrel_file = r\"MSMARCO_SMALL\\qrel_small.csv\"\n",
    "\n",
    "retrival_model = RetrievalModel(index)\n",
    "\n",
    "query_data = pd.read_csv(query_file)\n",
    "query_data['query'] = query_data['query'].apply(Index.preprocess_text)\n",
    "\n",
    "query = query_data['query'].iloc[0]\n",
    "# query = query_data.loc[query_data['qid'] == 722737, 'query'].iloc[0]\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.query_likelihood(query, 0.35)\n",
    "end_time = time.time()\n",
    "print(f\"query_likelihood took {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.bm25_ranking(query)\n",
    "end_time = time.time()\n",
    "print(f\"bm25_ranking took {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrival_model.embeddings_cosign_sim(query)\n",
    "end_time = time.time()\n",
    "print(f\"embeddings_cosign_sim took {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
